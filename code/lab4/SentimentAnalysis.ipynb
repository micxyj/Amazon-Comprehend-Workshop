{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 Amazon Comprehend 进行情感分析\n",
    "\n",
    "您可以使用 Amazon Comprehend 来确定文档的情绪。 您可以确定情绪是积极的、消极的、中性的还是混合的。 例如，您可以使用情感分析来确定对博客帖子的情感，以确定您的读者是否喜欢该帖子。\n",
    "\n",
    "可以使用 Amazon Comprehend 支持的任何主要语言执行该操作。 所有文件必须使用相同的语言。\n",
    "\n",
    "您可以使用以下任何操作来检测文档或一组文档的情绪。 \n",
    "\n",
    "    DetectSentiment\n",
    "\n",
    "    BatchDetectSentiment\n",
    "\n",
    "    StartSentimentDetectionJob\n",
    "\n",
    "这些操作返回文本最可能的情绪以及每个情绪的分数。 该分数表示正确检测到情绪的可能性。 例如，在下面的示例中，文本具有正面情绪的可能性为 95%。文本具有负面情绪的可能性不到 1%。您可以使用 SentimentScore 来确定检测的准确性是否满足您的应用程序的需求。\n",
    "\n",
    "DetectSentiment 操作返回一个包含检测到的情绪的对象和一个 SentimentScore 对象。 BatchDetectSentiment 操作返回一个情绪列表和每个情绪对应的SentimentScore 对象。 StartSentimentDetectionJob 操作启动一个异步作业，该作业生成一个包含情绪的列表和对应的 SentimentScore分数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验包括使用 Amazon Comprehend 执行情绪分析的步骤说明。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化\n",
    "\n",
    "首先需要指定：\n",
    "\n",
    "* AWS 区域。\n",
    "* IAM 角色 arn 用于授予对 Comprehend API 和 S3 存储桶的访问权限。 \n",
    "* 您要用于存放训练数据的 S3 存储桶。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if bucket exist\n",
    "\n",
    "sts_client = boto3.client(\"sts\")\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "print(\"Your account id is {}\".format(account_id))\n",
    "\n",
    "bucket = \"comprehend-labs\" + account_id +  \"-2\"\n",
    "print (\"Bucket name used is \" + bucket)\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "if (s3.Bucket(bucket).creation_date is None):\n",
    "    s3_client.create_bucket(Bucket=bucket)\n",
    "    print (\"Created bucket \" + bucket)\n",
    "else:\n",
    "    print (\"Bucket Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"sentiment-analysis\"\n",
    "bucketuri=\"s3://\"+bucket+\"/\"+prefix\n",
    "print(bucketuri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据\n",
    "我们首先将数据集上传到 s3 存储桶。样本数据集来自“Amazon reviews - Full”中提取的亚马逊评论，该数据集在\"Character-level Convolutional Networks for Text Classification\" (Xiang Zhang et al., 2015)一文当中被使用。\n",
    "\n",
    "现在，使用Pandas读取数据并进行查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data set\n",
    "\n",
    "!wget https://docs.aws.amazon.com/comprehend/latest/dg/samples/tutorial-reviews-data.zip\n",
    "!apt-get install unzip -y\n",
    "!unzip -o tutorial-reviews-data.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd \n",
    "\n",
    "# data = pd.read_csv('./amazon-reviews.csv')   \n",
    "data = pd.read_csv('./amazon-reviews.csv', header=None, names=['Review'])\n",
    "pd.set_option('display.max_rows', 20)# Keep the output on one page\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 detect_sentiment API 用于实时检测\n",
    "\n",
    "首先，我们将使用detect_sentiment API。 DetectSentiment 操作返回一个包含检测到的情绪的对象和一个 SentimentScore 对象。\n",
    "\n",
    "让我们检查一个纯文本示例开始。\n",
    "\n",
    "步骤：\n",
    "* 使用boto3初始化comprehend客户端\n",
    "* 定义示例文本\n",
    "* 调用detect_sentiment API 并将文本作为输入参数传入。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "comprehend = boto3.client(service_name='comprehend', region_name=region)\n",
    "                \n",
    "text = \"It is raining today in Seattle\"\n",
    "\n",
    "print('Calling DetectSentiment')\n",
    "print(json.dumps(comprehend.detect_sentiment(Text=text, LanguageCode='en'), sort_keys=True, indent=4))\n",
    "print('End of DetectSentiment\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在对示例数据集使用 detect_sentiment API 并检查响应。\n",
    "\n",
    "注意：我们只是测试了 5 条评论，然后检查输出 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iloc[:5].iterrows():\n",
    "    print(row[0])\n",
    "    print(\"\\n\")\n",
    "    print(json.dumps(comprehend.detect_sentiment(Text=row[0], LanguageCode='en'), sort_keys=True, indent=4))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 batch_detect_sentiment API\n",
    "您最多可以在一个batch api中发送 25 篇文档进行分析。调用批处理操作的方法与调用单个文档 API 进行处理操作的方法类似。使用批处理 API 可以为您的应用程序带来更好的性能。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will prepare a list of the 25 review document so we can use it for batch function\n",
    "rows,columns=data.shape\n",
    "\n",
    "list_text=[] #your empty list \n",
    "for index in range(25): #iteration over the dataframe\n",
    "    list_text.append(data.iat[index,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend.batch_detect_sentiment(\n",
    "    TextList=list_text,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 StartSentimentDetectionJob API 进行异步批处理操作\n",
    "\n",
    "要分析大型文档和大型文档集合，请使用 Amazon Comprehend 异步操作。\n",
    "\n",
    "要分析文档集合，您通常执行以下步骤：\n",
    "\n",
    "    * 将文档存储在 Amazon S3 存储桶中。\n",
    "\n",
    "    * 开始一项或多项工作来分析文件。\n",
    "\n",
    "    * 监控分析工作的进度。\n",
    "\n",
    "    * 作业完成后，从 S3 存储桶中检索分析结果。\n",
    "\n",
    "以下部分介绍了使用 Amazon Comprehend API 运行异步操作。\n",
    "\n",
    "我们将使用 StartSentimentDetectionJob API 以检测集合中每个文档中的情绪。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to s3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.Bucket(bucket).upload_file(\"amazon-reviews.csv\", \"{}/amazon-reviews.csv\".format(prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This name should match the name of the role that was created in the first lab.\n",
    "role_name_base = 'AmazonComprehendServiceRoleS3FullAccess-ComprehendLabs'\n",
    "prefix_random_numbers = '' #If you added random numbers to the end of the 'ComprehendLabs' prefix, put them here\n",
    "if not prefix_random_numbers:\n",
    "    role_name = \"{}{}\".format(role_name_base,prefix_random_numbers)\n",
    "else:\n",
    "    role_name = role_name_base\n",
    "iam_client = boto3.client(\"iam\")\n",
    "response = iam_client.get_role(\n",
    "    RoleName=role_name\n",
    ")\n",
    "comprehend_arn = response['Role']['Arn']\n",
    "print(\"The ARN for the role is {}\".format(comprehend_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"sentimentanalysis-job-{job_uuid}\"\n",
    "inputs3uri= bucketuri+\"/amazon-reviews.csv\"\n",
    "asyncresponse = comprehend.start_sentiment_detection_job(\n",
    "    InputDataConfig={\n",
    "        'S3Uri': inputs3uri,\n",
    "        'InputFormat': 'ONE_DOC_PER_LINE'\n",
    "    },\n",
    "    OutputDataConfig={\n",
    "        'S3Uri': bucketuri,\n",
    "       \n",
    "    },\n",
    "    DataAccessRoleArn=comprehend_arn,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en',\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check job status\n",
    "events_job_id = asyncresponse['JobId']\n",
    "job = comprehend.describe_sentiment_detection_job(JobId=events_job_id)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同时也可以在控制台上查看到任务状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "# Get current job status\n",
    "job = comprehend.describe_sentiment_detection_job(JobId=events_job_id)\n",
    "print(job)\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 10\n",
    "while job['SentimentDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(60)\n",
    "    waited += 60\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    job = comprehend.describe_sentiment_detection_job(JobId=events_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 该作业大约需要 6-8 分钟才能完成，您可以从作业参数中指定的输出位置下载输出。 \n",
    "#### 您可以在控制台中打开 Comprehend 并在那里查看作业的详细信息。 当您有多个文档并且想要运行异步批处理时，该方法将非常有用。 "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
